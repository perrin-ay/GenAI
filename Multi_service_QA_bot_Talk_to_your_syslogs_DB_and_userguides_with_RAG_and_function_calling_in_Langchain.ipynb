{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z42_UqPDAhsI"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install langchain huggingface_hub\n",
        "\n",
        "!{sys.executable} -m  pip install bitsandbytes>=0.39.0\n",
        "!{sys.executable} -m pip install --upgrade accelerate\n",
        "!{sys.executable} -m pip install --upgrade sentence_transformers\n",
        "!{sys.executable} -m  pip install --upgrade transformers\n",
        "!{sys.executable} -m pip install --upgrade trl\n",
        "!{sys.executable} -m pip install --upgrade peft\n",
        "!{sys.executable} -m pip install --upgrade sqlite3\n",
        "!{sys.executable} -m pip install --upgrade pytz\n",
        "!{sys.executable} -m pip install --upgrade pypdf PyPDF2\n",
        "!{sys.executable} -m pip install --upgrade langchain_experimental\n",
        "!{sys.executable} -m pip install --upgrade langchain_community\n",
        "!{sys.executable} -m pip install --upgrade langchain_openai\n",
        "!{sys.executable} -m pip install --upgrade openai tiktoken chromadb\n",
        "!pip install unstructured['pdf'] unstructured\n",
        "!{sys.executable} -m pip install --upgrade textwrap\n",
        "!pip install --upgrade --quiet  cohere\n",
        "!pip install langchain-cohere\n",
        "!pip install grandalf\n",
        "!pip install wikipedia\n",
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu\n",
        "!pip install langchainhub\n",
        "!pip install -qU langchain-mistralai\n",
        "!pip install \"unstructured[html]\"\n",
        "!pip install bs4\n",
        "!pip install rank_bm25\n",
        "!pip install gradio\n",
        "\n",
        "import gradio as gr\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "import os\n",
        "import time\n",
        "import sqlite3\n",
        "import re\n",
        "from pytz import timezone\n",
        "import pytz\n",
        "import datetime\n",
        "import json\n",
        "import textwrap\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch\n",
        "from operator import itemgetter\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import peft\n",
        "import bitsandbytes\n",
        "import accelerate\n",
        "import trl\n",
        "from accelerate import init_empty_weights\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoConfig, BitsAndBytesConfig\n",
        "from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model\n",
        "from accelerate import load_checkpoint_and_dispatch\n",
        "import accelerate, bitsandbytes\n",
        "\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "from langchain import hub\n",
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "from langchain_experimental.agents import create_csv_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, PyMuPDFLoader\n",
        "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain.tools import BaseTool, StructuredTool, tool\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain.agents import create_tool_calling_agent\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain.agents import AgentType, initialize_agent\n",
        "from langchain_core.messages import HumanMessage, ToolMessage, SystemMessage, AIMessage\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate your llm - using Mistral in this example"
      ],
      "metadata": {
        "id": "LPUfCnVNCl_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Mistral_TOKEN = userdata.get('mistralapi')\n",
        "os.environ['MISTRAL_API_KEY'] = Mistral_TOKEN\n",
        "llmMist = ChatMistralAI(model=\"mistral-large-latest\", temperature =0)"
      ],
      "metadata": {
        "id": "Wi07eInPBvki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to ingest Appwall security and system DBs in this example and return pandas dataframes"
      ],
      "metadata": {
        "id": "t0NFq-7fCti9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def epochtotzone(ep,tzo= None):\n",
        "  \"\"\"\n",
        "  Convert epoch values in db to DateTime\n",
        "  \"\"\"\n",
        "  if not tzo:\n",
        "    tzo = \"Asia/Jerusalem\"\n",
        "  tzn=timezone(tzo)\n",
        "  try:\n",
        "      t=datetime.datetime.utcfromtimestamp(ep).replace(tzinfo=pytz.utc)\n",
        "      x= t.astimezone(tzn)\n",
        "      return t.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
        "\n",
        "\n",
        "def aw_to_pd(f,ftype='security',tzone=None):\n",
        "  \"\"\"\n",
        "  Appwall security and system db to pandas dataframe\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  jsonlist=[]\n",
        "\n",
        "  if 'security' in ftype:\n",
        "      colls= ['DateTime','TargetPort','TargetIP','TransID','TunnelID','IsPassiveMode','Title','URI','Description']\n",
        "  elif 'system' in ftype:\n",
        "      colls=['DateTime','Title','Description']\n",
        "\n",
        "  con = sqlite3.connect(f)\n",
        "  con.text_factory = lambda b: b.decode(errors = 'ignore') # added to ignore utf-8 decode failure\n",
        "  cursor = con.cursor()\n",
        "  cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "  qu='SELECT * from %s'%'Events'\n",
        "  df=pd.read_sql(qu,con)\n",
        "  df=df[~df['DateTime'].isna()]\n",
        "  df['DateTime']=df['DateTime'].apply(epochtotzone,args=(tzone,))\n",
        "  df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
        "  df=df[colls]\n",
        "  df['Date']= df['DateTime']\n",
        "  df = df.drop('DateTime', axis=1, inplace=False)\n",
        "  df['Date'] = df['Date'].astype(str)\n",
        "  if 'security' in ftype:\n",
        "      df['TransID'] = df['TransID'].astype(str)\n",
        "  return df\n",
        "\n",
        "gdrivepath = \"/content/drive/MyDrive/Colab Notebooks/My NN stuff/alteon/dbe.security.db\"\n",
        "\n",
        "df_aw = aw_to_pd(gdrivepath)\n",
        "\n"
      ],
      "metadata": {
        "id": "A1TH1CHXBzDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting up langchain components and pipeline for QA with your db using function calling**"
      ],
      "metadata": {
        "id": "7-gsZ2jKC7hU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python_repl_appwall = PythonREPL()\n",
        "python_repl_appwall.globals['df'] = df_aw\n",
        "\n",
        "\n",
        "# Permanently changes the pandas settings\n",
        "python_repl_appwall.run(pd.set_option('display.max_rows', None))\n",
        "python_repl_appwall.run(pd.set_option('display.max_columns', None))\n",
        "python_repl_appwall.run(pd.set_option('display.width', None))\n",
        "python_repl_appwall.run(pd.set_option('display.max_colwidth', 500))\n",
        "\n",
        "\n",
        "name_appwall = \"python_repl_for_pandas_dataframes_appwall_database\"\n",
        "\n",
        "description_appwall_2= \"\"\"\n",
        "A Python shell to execute pandas commands on appwall dataframe 'df' which contains appwall database (db).\n",
        "The column names in this pandas dataframe df can be found in this list = ['Date','TargetPort','TargetIP','TransID','TunnelID','IsPassiveMode','Title','URI','Description']\n",
        "The dataframe df contains entries in increasing order of time,\n",
        "such that the latest entry is at the end and earliest entry is at the beginning.\n",
        "The final answer should always be enclosed in a print() function\n",
        "\"\"\"\n",
        "\n",
        "repl_tool_appwall = Tool(\n",
        "    name=name_appwall,\n",
        "    description=description_appwall_2,\n",
        "    func=python_repl_appwall.run\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "appwall_llm = llmMist.bind_tools([repl_tool_appwall])\n",
        "\n",
        "sys_appwall= \"\"\"\n",
        "Only if user query is talking about 'appwall' or 'WAF' or 'AW' or 'CWAF' or 'database' or 'db',\n",
        "do they have questions about the data stored in the pandas dataframe df for appwall database.\n",
        "\"\"\"\n",
        "\n",
        "prompt_appwall = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", sys_appwall),\n",
        "        (\"human\", \"{query}\"),\n",
        "        ])\n",
        "\n",
        "setupinputs = {\"query\": RunnablePassthrough()}\n",
        "\n",
        "\n",
        "def check_print(result: str) -> str:\n",
        "  if 'print' in result:\n",
        "    return result\n",
        "  else:\n",
        "    return f\"print({result})\"\n",
        "\n",
        "def out_parser_appwall(result: str) -> str:\n",
        "\n",
        "  \"\"\"Parses the output of python repl and returns the final formatted output\"\"\"\n",
        "\n",
        "  no_answer = \"I couldnt locate what you are asking for in the appwall db.\"\n",
        "  attributeerr = \"I dont have an answer to your query. Please try to revise your question and ask again. \"\n",
        "  if not result:\n",
        "    return no_answer\n",
        "  if 'Empty DataFrame' in result:\n",
        "    return no_answer\n",
        "  if 'AttributeError' in result:\n",
        "    return attributeerr\n",
        "  return result\n",
        "\n",
        "def toolingfunc_appwall(x: str) -> str:\n",
        "  try:\n",
        "    return x.tool_calls[0][\"args\"]['__arg1']\n",
        "  except:\n",
        "    # No tool/arg found. This need to be different message encouraging user to rephrase the question\n",
        "    return \"print('I dont have an answer to your query. Kindly try rephrasing your question or adding more specificity to it.')\"\n",
        "\n",
        "\n",
        "def notebook_rate_print(data: str) -> str:\n",
        "  if not data:\n",
        "    return data\n",
        "  data_size = len(data)\n",
        "  if data_size > 999000:\n",
        "    return \"Response data size exceeds display limit and will only be partially displayed. Please refine your query and ask again!\\n\\n\" +data[:999000]\n",
        "  else:\n",
        "    return data\n",
        "\n",
        "appwall_chain = setupinputs| prompt_appwall | appwall_llm | RunnableLambda(toolingfunc_appwall) | RunnableLambda(check_print) | python_repl_appwall.run | RunnableLambda(notebook_rate_print) | RunnableLambda(out_parser_appwall)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HT9Yje1_CSRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load up your alteon syslogs and convert to pandas dataframe**"
      ],
      "metadata": {
        "id": "EpW-_k-TDHFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"alteon_syslogs.txt\") as fd:\n",
        "  txtdocuments = fd.read()\n",
        "\n",
        "txts = txtdocuments.splitlines()\n",
        "txts = [i[i.find(\",2024\")+1:] for i in txts]\n",
        "dii = {\"syslogs\":txts}\n",
        "df_syslogs = pd.DataFrame(dii)\n",
        "\n",
        "\n",
        "##### prepending each entry with Month-day-year-time #################\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "def prependdt(row):\n",
        "  # Regular expression pattern to match the datetime format\n",
        "  pattern = r\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}[\\+\\-]\\d{2}:\\d{2}\"\n",
        "  time_str = re.findall(pattern, row)\n",
        "  if len(time_str) > 0:\n",
        "    time_str= time_str[0]\n",
        "    #time_str = \"2024-02-08T15:16:40+05:30\"\n",
        "\n",
        "    # Parse the string with format specifier considering timezone offset\n",
        "    datetime_obj = datetime.strptime(time_str, \"%Y-%m-%dT%H:%M:%S%z\")\n",
        "\n",
        "    # Format the datetime object as desired (Month-day-year-time)\n",
        "    formatted_time = datetime_obj.strftime(\"%B-%d-%Y-%H:%M:%S\")\n",
        "\n",
        "    return formatted_time + \" \" + row\n",
        "  else:\n",
        "    print (\"emty timestr: \", time_str, row)\n",
        "    return row\n",
        "\n",
        "df_syslogs['syslogs'] = df_syslogs['syslogs'].apply(prependdt)\n",
        "\n",
        "\n",
        "########## convert to string lower for all data in syslog ###########\n",
        "\n",
        "df_syslogs['syslogs'] = df_syslogs['syslogs'].str.lower()\n"
      ],
      "metadata": {
        "id": "hHRWn0rRDGpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup syslog langchain components and pipeline**"
      ],
      "metadata": {
        "id": "Rzolfm6HmXPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python_repl_syslogs = PythonREPL()\n",
        "python_repl_syslogs.globals['df'] = df_syslogs\n",
        "\n",
        "# Permanently changes the pandas settings\n",
        "python_repl_syslogs.run(pd.set_option('display.max_rows', None))\n",
        "python_repl_syslogs.run(pd.set_option('display.max_columns', None))\n",
        "python_repl_syslogs.run(pd.set_option('display.width', None))\n",
        "python_repl_syslogs.run(pd.set_option('display.max_colwidth', 500))\n",
        "\n",
        "name_syslogs = \"python_repl_for_pandas_dataframes_alteon_syslogs\"\n",
        "\n",
        "description_syslogs= \"\"\"\n",
        "A Python shell to execute pandas commands on dataframe 'df' which contains alteon syslogs.\n",
        "There is only one column in this df called 'syslogs'.\n",
        "The alteon syslogs are stored in df['syslogs'] in increasing order of time,\n",
        "such that the latest syslog is at the end and earliest syslog is at the beginning.\n",
        "Output should be only the pandas commands converted to dict using to_dict() and no other words.\n",
        "The final answer should always be enclosed in a print() function\n",
        "\"\"\"\n",
        "\n",
        "repl_tool_syslogs = Tool(\n",
        "    name=name_syslogs,\n",
        "    description=description_syslogs,\n",
        "    func=python_repl_syslogs.run\n",
        "\n",
        ")\n",
        "\n",
        "syslogs_llm = llmMist.bind_tools([repl_tool_syslogs])\n",
        "\n",
        "sys_syslogs=\"\"\"\n",
        "You are an AI assistant that take user queries about alteon syslogs and events as input and only returns pandas commands to execute on dataframe 'df' which contains alteon syslogs.\n",
        "There is only one column in this df called 'syslogs'.\n",
        "The alteon syslogs are stored in df['syslogs'] in increasing order of time,\n",
        "such that the latest syslog is at the end and earliest syslog is at the beginning.\n",
        "The pandas commands should be case-insensitive.\n",
        "Output should be only the pandas commands converted to dict using to_dict() and no other words.\n",
        "The final answer should always be enclosed in a print() function\n",
        "\"\"\"\n",
        "prompt_syslogs = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", sys_syslogs),\n",
        "        (\"human\", \"{query}\"),\n",
        "        ])\n",
        "\n",
        "\n",
        "def check_print(result: str) -> str:\n",
        "  if 'print' in result:\n",
        "    return result\n",
        "  else:\n",
        "    return f\"print({result})\"\n",
        "\n",
        "def out_parser_syslogs(result: str) -> str:\n",
        "\n",
        "  \"\"\"Parses the output of python repl and returns the final formatted output\"\"\"\n",
        "  no_answer = \"I couldnt locate what you are asking for in the alteon syslogs\"\n",
        "\n",
        "  if not result:\n",
        "    return no_answer\n",
        "  if \"df['syslogs']\" in result:\n",
        "    return no_answer\n",
        "  finalls = []\n",
        "  outtype = None\n",
        "\n",
        "  if result[0] ==\"[\":\n",
        "    outtype = \"list\"\n",
        "  elif result[0] == \"{\":\n",
        "    outtype = \"dict\"\n",
        "  else:\n",
        "    outtype = \"string\"\n",
        "\n",
        "  if outtype ==\"list\":\n",
        "\n",
        "    ls = list(eval(result))\n",
        "    if not ls:\n",
        "      return no_answer\n",
        "    if isinstance(ls[0], dict):\n",
        "      for i in ls:\n",
        "        finalls.append(\"\".join(i.values()))\n",
        "      if finalls:\n",
        "        return \"\\n\".join(finalls)\n",
        "      else:\n",
        "        return no_answer\n",
        "    else:\n",
        "      for i in ls:\n",
        "        finalls.append(i)\n",
        "      if finalls:\n",
        "        return \"\\n\".join(finalls)\n",
        "      else:\n",
        "        return no_answer\n",
        "\n",
        "  elif outtype ==\"dict\":\n",
        "\n",
        "    ls = dict(eval(result))\n",
        "    if not ls:\n",
        "      return no_answer\n",
        "    for i in ls:\n",
        "      if isinstance(ls[i], dict):\n",
        "        for j in ls[i]:\n",
        "          finalls.append(ls[i][j])\n",
        "        if finalls:\n",
        "          return \"\\n\".join(finalls)\n",
        "        else:\n",
        "          return no_answer\n",
        "      else:\n",
        "        for i in ls:\n",
        "          finalls.append(ls[i])\n",
        "        if finalls:\n",
        "          return \"\\n\".join(finalls)\n",
        "        else:\n",
        "          return no_answer\n",
        "\n",
        "  elif outtype == \"string\":\n",
        "    return result\n",
        "\n",
        "def toolingfunc_syslogs(x: str) -> str:\n",
        "  try:\n",
        "    return x.tool_calls[0][\"args\"]['__arg1']\n",
        "  except:\n",
        "    # No tool/arg found. This need to be different message encouraging user to rephrase the question\n",
        "    return \"print('I dont have an answer to your query. Kindly try rephrasing your question or adding more specificity to it.')\"\n",
        "\n",
        "def extractcontentonly(x) -> str:\n",
        "  if not x.content:\n",
        "    return \"print('I couldnt find an answer to your query. Kindly try rephrasing your question or adding more specificity to it.')\"\n",
        "  try:\n",
        "    return x.content\n",
        "  except:\n",
        "    # No tool/arg found. This need to be different message encouraging user to rephrase the question\n",
        "    return \"print('I dont have an answer to your query. Kindly try rephrasing your question or adding more specificity to it.')\"\n",
        "\n",
        "def notebook_rate_print(data: str) -> str:\n",
        "\n",
        "  if not data:\n",
        "    return data\n",
        "  data_size = len(data)\n",
        "  if data_size > 999000:\n",
        "    return \"Response data size exceeds display limit and will only be partially displayed. Please refine your query and ask again!\\n\\n\" +data[:999000]\n",
        "  else:\n",
        "    return data\n",
        "\n",
        "setupinputs = {\"query\": RunnablePassthrough()}\n",
        "\n",
        "syslogs_chain = setupinputs| prompt_syslogs | syslogs_llm | RunnableLambda(toolingfunc_syslogs) | RunnableLambda(check_print) | python_repl_syslogs.run | RunnableLambda(out_parser_syslogs) |RunnableLambda(notebook_rate_print)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yXU1p5PHmWhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load up chroma vector db with REST API guide chunk embeddings**"
      ],
      "metadata": {
        "id": "ibEcDB8foT-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
        "model_kwargs = {'device': 'cpu', 'trust_remote_code': True}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "!unzip \"/content/drive/MyDrive/Colab Notebooks/My NN stuff/alteon/chroma_db_rest.zip\"\n",
        "persist_directory = 'fulldb'\n",
        "vectordb_v1 = Chroma(persist_directory=persist_directory, embedding_function=hf)\n",
        "print(vectordb_v1._collection.count())"
      ],
      "metadata": {
        "id": "jJUIhD4doTiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZlBENaEOoVMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template6 = \"\"\"Given the context information below and not prior knowledge, answer the question.\n",
        "The context provides REST and Web API information on the Alteon.\n",
        "Think step by step before answering the question.\n",
        "The user is most interested in the URLs found in the context.\n",
        "From the context provided, extract all relevant URLs and descriptions to those URLs ( if any ), that possibly answer the question.\n",
        "The final answer should not change or modify the URLs and can contain several URLs.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "#Check if there is anything extraneous in the final answer that can be removed.\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\",\"query\"],\n",
        "    template=template6\n",
        ")\n",
        "\n",
        "\n",
        "retriever_v1 = vectordb_v1.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
        "compressor = CohereRerank(top_n=7)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever_v1\n",
        ")\n",
        "\n",
        "\n",
        "def get_citations(docs):\n",
        "  cites = []\n",
        "  for doc in docs:\n",
        "    cites.append(doc.metadata['source'])\n",
        "  return \"\\n\\n\".join(cites)\n",
        "\n",
        "def return_only_answer_source_docs(response, citations_only= True):\n",
        "  cites = []\n",
        "  docs= response[\"context\"]\n",
        "  for doc in docs:\n",
        "    cites.append(doc.metadata['source'])\n",
        "\n",
        "  idx = response[\"answer\"].find(\"Answer:\")\n",
        "  if citations_only:\n",
        "    respstr = response[\"answer\"][idx+7:] + \"\\n\\nSources:\\n\\n\" + \"\\n\".join(cites)\n",
        "  else:\n",
        "    respstr = response\n",
        "  return respstr\n",
        "\n",
        "\n",
        "\n",
        "rag_chain_from_docs = (\n",
        "    prompt_template\n",
        "    | llmMist\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "setupandret = RunnableParallel({\"context\": compression_retriever, \"query\": RunnablePassthrough()})\n",
        "rag_chain_with_source = setupandret.assign(answer=rag_chain_from_docs)\n",
        "RESTapi_chain = rag_chain_with_source | RunnableLambda(return_only_answer_source_docs)\n"
      ],
      "metadata": {
        "id": "TJ6q2IstoTks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keyword router**"
      ],
      "metadata": {
        "id": "-hjs1mxhmWrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_classifier(query):\n",
        "\n",
        "  q = query.lower()\n",
        "  cls = {'class':'','query':query}\n",
        "\n",
        "  if 'rest' in q or 'api call' in q or 'ui' in q or 'gui' in q or 'api' in q or 'url' in q:\n",
        "    cls['class'] = 'RESTapi'\n",
        "    return cls\n",
        "\n",
        "  elif 'appwall' in q or 'cwaf' in q or 'waf' in q or 'database' in q or 'db' in q or 'aw' in q:\n",
        "    cls['class'] = 'appwall'\n",
        "    return cls\n",
        "\n",
        "  elif 'syslog' in q or 'alteon syslog' in q or 'alteon log' in q:\n",
        "    cls['class'] = 'syslogs'\n",
        "    return cls\n",
        "\n",
        "  else:\n",
        "    cls['class'] = 'Other'\n",
        "    return cls\n",
        "\n",
        "def chain_selector(clsdict):\n",
        "\n",
        "  otherans = \"\"\"Hello! I am a chat service to talk to Alteon syslogs, or Appwall db. You can also ask me how to make REST calls to the Alteon.\n",
        "Please phrase your query indicating which service you are interested in and ask again.\n",
        "For example if you want to talk to Alteon syslogs , you can trying asking:\n",
        "Are there any cpu events in the alteon syslogs ?\n",
        "  \"\"\"\n",
        "\n",
        "  if clsdict['class'] == 'RESTapi':\n",
        "    return RESTapi_chain.invoke(clsdict['query'])\n",
        "\n",
        "  elif clsdict['class'] == 'appwall':\n",
        "    return appwall_chain.invoke({'query':clsdict['query']})\n",
        "\n",
        "  elif clsdict['class'] == 'syslogs':\n",
        "    return syslogs_chain.invoke({'query':clsdict['query'].lower()})\n",
        "\n",
        "  else:\n",
        "    return otherans\n",
        "\n",
        "full_chain = RunnableLambda(query_classifier) | RunnableLambda(chain_selector)\n",
        "\n",
        "def chained_qa(message, history):\n",
        "  return full_chain.invoke(message)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FPrGurp9nzVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradio app for web interface**"
      ],
      "metadata": {
        "id": "Mi2EizRwoJI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    gr.ChatInterface(\n",
        "        chained_qa,\n",
        "        chatbot=gr.Chatbot(height=300),\n",
        "        textbox=gr.Textbox(placeholder=\"Talk to Alteon syslogs, WAF events or ask me how to make Alteon REST API calls\", container=False, scale=7),\n",
        "        title=\"Radware Services\",\n",
        "#        description=\"Alteon syslogs, WAF events & Alteon REST API\",\n",
        "        theme=\"soft\",\n",
        "        examples= [\"In AW DB how many entries under Description column contain AllowList\",\n",
        "                   \"Are there any CPU events in the alteon syslogs ?\",\n",
        "                   \"How to check Alteon syslog table via rest call?\"],\n",
        "        cache_examples=False,\n",
        "        retry_btn=None,\n",
        "        undo_btn=\"Delete Previous\",\n",
        "        clear_btn=\"Clear\",\n",
        "    ).launch(share= True)"
      ],
      "metadata": {
        "id": "MuHE4fNwnzgV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
