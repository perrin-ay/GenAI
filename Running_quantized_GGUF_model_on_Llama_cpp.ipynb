{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Download quantized model from huggingface hub**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "12aRWrfpb-Mg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD6qFV-JaFZ5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "def download_mpt_quant(destination_folder: str, repo_id: str, model_filename: str):\n",
        "    local_path = os.path.abspath(destination_folder)\n",
        "    return hf_hub_download(\n",
        "        repo_id=repo_id,\n",
        "        filename=model_filename,\n",
        "        local_dir=local_path,\n",
        "        local_dir_use_symlinks=True\n",
        "    )\n",
        "\n",
        "\n",
        "#repo_id = \"TheBloke/mpt-30B-chat-GGML\"\n",
        "#model_filename = \"mpt-30b-chat.ggmlv0.q4_1.bin\"\n",
        "repo_id = \"MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF\"\n",
        "model_filename = \"Meta-Llama-3-8B-Instruct.Q5_K_M.gguf\"\n",
        "destination_folder = \"models\"\n",
        "download_mpt_quant(destination_folder, repo_id, model_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Llama cpp**"
      ],
      "metadata": {
        "id": "22u4jJG2bzhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for cpu and gpu\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir\n",
        "#just for cpu\n",
        "#!pip install --upgrade --quiet  llama-cpp-python"
      ],
      "metadata": {
        "id": "JnSxHqGZadzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load GGUF quantized model on Llamma cpp for for inference**\n"
      ],
      "metadata": {
        "id": "XyULNJlFakZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "llm = Llama(model_path=\"/content/models/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf\",\n",
        "            n_gpu_layers=-1) # all layers in gpu\n",
        "print (llm)\n",
        "response = llm(\"Explain modal epistemology?\", max_tokens=1000)\n",
        "response"
      ],
      "metadata": {
        "id": "nnvJQuAjajuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Langchain wrapper for inference**"
      ],
      "metadata": {
        "id": "RyR1GWYOazl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "\n",
        "n_gpu_layers = -1\n",
        "n_batch = 512\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/models/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf\",\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "llm_chain = prompt | llm\n",
        "question = \"Explain modal epistemology?\"\n",
        "llm_chain.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "I-sy7GUOazPb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}