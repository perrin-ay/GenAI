{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySx13a8I4htn"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install langchain huggingface_hub\n",
        "\n",
        "!{sys.executable} -m  pip install bitsandbytes>=0.39.0\n",
        "!{sys.executable} -m pip install --upgrade accelerate\n",
        "!{sys.executable} -m pip install --upgrade sentence_transformers\n",
        "!{sys.executable} -m  pip install --upgrade transformers\n",
        "!{sys.executable} -m pip install --upgrade trl\n",
        "!{sys.executable} -m pip install --upgrade peft\n",
        "!{sys.executable} -m pip install --upgrade sqlite3\n",
        "!{sys.executable} -m pip install --upgrade pytz\n",
        "!{sys.executable} -m pip install --upgrade pypdf PyPDF2\n",
        "!{sys.executable} -m pip install --upgrade langchain_experimental\n",
        "!{sys.executable} -m pip install --upgrade langchain_community\n",
        "!{sys.executable} -m pip install --upgrade langchain_openai\n",
        "!{sys.executable} -m pip install --upgrade openai tiktoken chromadb\n",
        "!pip install unstructured['pdf'] unstructured\n",
        "!{sys.executable} -m pip install --upgrade textwrap\n",
        "!pip install --upgrade --quiet  cohere\n",
        "!pip install langchain-cohere\n",
        "!pip install grandalf\n",
        "!pip install wikipedia\n",
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu\n",
        "!pip install langchainhub\n",
        "!pip install -qU langchain-mistralai\n",
        "!pip install \"unstructured[html]\"\n",
        "!pip install bs4\n",
        "!pip install rank_bm25\n",
        "!pip install gradio\n",
        "!pip install langchain_chroma\n",
        "\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "import os\n",
        "import time\n",
        "import sqlite3\n",
        "import re\n",
        "from pytz import timezone\n",
        "import pytz\n",
        "import datetime\n",
        "import json\n",
        "import textwrap\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch\n",
        "from operator import itemgetter\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import peft\n",
        "import bitsandbytes\n",
        "import accelerate\n",
        "import trl\n",
        "from accelerate import init_empty_weights\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoConfig, BitsAndBytesConfig\n",
        "from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model\n",
        "from accelerate import load_checkpoint_and_dispatch\n",
        "import accelerate, bitsandbytes\n",
        "\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "from langchain import hub\n",
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "from langchain_experimental.agents import create_csv_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, PyMuPDFLoader\n",
        "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain.tools import BaseTool, StructuredTool, tool\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain.agents import create_tool_calling_agent\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain.agents import AgentType, initialize_agent\n",
        "from langchain_core.messages import HumanMessage, ToolMessage, SystemMessage, AIMessage\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        ")\n",
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLM instantiation**"
      ],
      "metadata": {
        "id": "TiDrJM7D7UAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Mistral_TOKEN = userdata.get('mistralapi')\n",
        "os.environ['MISTRAL_API_KEY'] = Mistral_TOKEN\n",
        "llmMist = ChatMistralAI(model=\"mistral-large-latest\", temperature =0)"
      ],
      "metadata": {
        "id": "AbWinGEN7TvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings model**"
      ],
      "metadata": {
        "id": "fpgJ6qf-7bpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
        "model_kwargs = {'device': 'cpu', 'trust_remote_code': True}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "XjjfTZ507bLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create embeddings to use in example selector**\n",
        "\n",
        "- see data folder for the pickle files listed in below cell\n",
        "- the instruction set in the pkl files for the example selector look like\n",
        "\n",
        "[\n",
        "    {\"instruction\": \"commands for an alteon virt with ip 10.106.34.3 and service https\",\n",
        "     \"alteon-command\": \"/c/slb/virt 1/vip 10.106.34.3/service 443 https\"},\n",
        "    {\"instruction\": \"commands for an alteon virt with id test and ip address 8.89.88.4 and service FTP\",\n",
        "     \"alteon-command\": \"/c/slb/virt test/vip 8.89.88.4/service FTP\"},\n",
        "    {\"instruction\": \"how to create alteon virtual server called arnab with vip 1.1.1.1 and service http\",\n",
        "     \"alteon-command\": \"/c/slb/virt arnab/vip 1.1.1.1/service http\"},\n",
        "    {\"instruction\": \"what are the commands for an alteon virt named idli with vip 10.4.5.3 and service https\",\n",
        "     \"alteon-command\": \"/c/slb/virt idli/vip 10.4.5.3/service 443 https\"}\n",
        "....]"
      ],
      "metadata": {
        "id": "P2sVEhTG7DnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "ls = []\n",
        "\n",
        "files  = ['instructionsetalteon_virt124.pkl', 'instructionsetalteon_virtip124.pkl',\n",
        "          'instructionsetalteon_virtgrp124.pkl', 'instructionsetalteon_real.pkl',\n",
        "          'instructionsetalteon_grp124.pkl']\n",
        "\n",
        "for f in files:\n",
        "\n",
        "  with open(f, 'rb') as f:\n",
        "    mastervirt = pickle.load(f)\n",
        "  df = pd.DataFrame(mastervirt)\n",
        "  df= df.sample(8)\n",
        "  tmpls = [ {'instruction': i[0].strip(\"\\n\"),'alteon-command':i[1]} for i in df.values]\n",
        "  ls.extend(tmpls)\n"
      ],
      "metadata": {
        "id": "2pfd3uEo6i-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorings all examples and store in Chroma**"
      ],
      "metadata": {
        "id": "RDxV9lWk7OUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.example_selectors import (\n",
        "    MaxMarginalRelevanceExampleSelector,\n",
        "    SemanticSimilarityExampleSelector,\n",
        ")\n",
        "to_vectorize = [\" \".join(l.values()) for l in ls]\n",
        "vectorstore = Chroma.from_texts(to_vectorize, hf, metadatas=ls)\n",
        "\n"
      ],
      "metadata": {
        "id": "JzG5Euf66jCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instantiate retreiver modes for semantic search**"
      ],
      "metadata": {
        "id": "VN92OCT9NBiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_selector = SemanticSimilarityExampleSelector(\n",
        "    vectorstore=vectorstore,\n",
        "    k=8,\n",
        ")\n",
        "example_selector_mmr = MaxMarginalRelevanceExampleSelector(\n",
        "    vectorstore=vectorstore,\n",
        "    k=8,\n",
        ")"
      ],
      "metadata": {
        "id": "kPWmVf_K6jMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Lanchain components and finalize pipeline**"
      ],
      "metadata": {
        "id": "qzz5TpFLOAA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"instruction\", \"alteon-command\"],\n",
        "    template=\"instruction: {instruction}\\nalteon-command: {alteon-command}\",\n",
        ")\n",
        "\n",
        "\n",
        "prefix = \"\"\"The following are examples of instructions that have been translated to alteon commands.\n",
        "The response should only include the alteon command for the instruction and nothing else.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "similar_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector_mmr,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=\"Instruction: {instruction}\\nalteon-command:\",\n",
        "    input_variables=[\"instruction\"],\n",
        "    example_separator=\"\\n\",\n",
        ")\n",
        "\n",
        "query = \"commands to construct an alteon virt called arnab with vip 8.8.8.9 and service basic-slb\"\n",
        "\n",
        "chain = similar_prompt | llmMist\n",
        "\n",
        "chain.invoke({\"instruction\": query}).content\n"
      ],
      "metadata": {
        "id": "t-6_ogiY6jQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradio web interface**"
      ],
      "metadata": {
        "id": "ScOA0P2DNqmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "def chained_qa(message, history):\n",
        "  return chain.invoke({\"instruction\": message}).content\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    gr.ChatInterface(\n",
        "        chained_qa,\n",
        "        chatbot=gr.Chatbot(height=300),\n",
        "        textbox=gr.Textbox(placeholder=\"ALTEON CLI COMMANDER\", container=False, scale=7),\n",
        "        title=\"Radware Services\",\n",
        "#        description=\"Ask for Alteon CLI commands\",\n",
        "        theme=\"soft\",\n",
        "        examples= [\"command to create an alteon virtual server called orco, with vip address 10.5.4.33 and service http\",\n",
        "                   \"commands to construct an alteon virt called arnab with vip 8.8.8.9 and service basic-slb\"],\n",
        "        cache_examples=False,\n",
        "        retry_btn=None,\n",
        "        undo_btn=\"Delete Previous\",\n",
        "        clear_btn=\"Clear\",\n",
        "    ).launch(share= True, debug=True)\n"
      ],
      "metadata": {
        "id": "Tej6ugHb6jT_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}