{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z42_UqPDAhsI"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install langchain huggingface_hub\n",
        "\n",
        "!{sys.executable} -m  pip install bitsandbytes>=0.39.0\n",
        "!{sys.executable} -m pip install --upgrade accelerate\n",
        "!{sys.executable} -m pip install --upgrade sentence_transformers\n",
        "!{sys.executable} -m  pip install --upgrade transformers\n",
        "!{sys.executable} -m pip install --upgrade trl\n",
        "!{sys.executable} -m pip install --upgrade peft\n",
        "!{sys.executable} -m pip install --upgrade sqlite3\n",
        "!{sys.executable} -m pip install --upgrade pytz\n",
        "!{sys.executable} -m pip install --upgrade pypdf PyPDF2\n",
        "!{sys.executable} -m pip install --upgrade langchain_experimental\n",
        "!{sys.executable} -m pip install --upgrade langchain_community\n",
        "!{sys.executable} -m pip install --upgrade langchain_openai\n",
        "!{sys.executable} -m pip install --upgrade openai tiktoken chromadb\n",
        "!pip install unstructured['pdf'] unstructured\n",
        "!{sys.executable} -m pip install --upgrade textwrap\n",
        "!pip install --upgrade --quiet  cohere\n",
        "!pip install langchain-cohere\n",
        "!pip install grandalf\n",
        "!pip install wikipedia\n",
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu\n",
        "!pip install langchainhub\n",
        "!pip install -qU langchain-mistralai\n",
        "!pip install \"unstructured[html]\"\n",
        "!pip install bs4\n",
        "!pip install rank_bm25\n",
        "!pip install gradio\n",
        "\n",
        "import gradio as gr\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "import os\n",
        "import time\n",
        "import sqlite3\n",
        "import re\n",
        "from pytz import timezone\n",
        "import pytz\n",
        "import datetime\n",
        "import json\n",
        "import textwrap\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch\n",
        "from operator import itemgetter\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import peft\n",
        "import bitsandbytes\n",
        "import accelerate\n",
        "import trl\n",
        "from accelerate import init_empty_weights\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoConfig, BitsAndBytesConfig\n",
        "from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model\n",
        "from accelerate import load_checkpoint_and_dispatch\n",
        "import accelerate, bitsandbytes\n",
        "\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "from langchain import hub\n",
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "from langchain_experimental.agents import create_csv_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, PyMuPDFLoader\n",
        "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain.tools import BaseTool, StructuredTool, tool\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain.agents import create_tool_calling_agent\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain.agents import AgentType, initialize_agent\n",
        "from langchain_core.messages import HumanMessage, ToolMessage, SystemMessage, AIMessage\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate your llm - using Mistral in this example"
      ],
      "metadata": {
        "id": "LPUfCnVNCl_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Mistral_TOKEN = userdata.get('mistralapi')\n",
        "os.environ['MISTRAL_API_KEY'] = Mistral_TOKEN\n",
        "llmMist = ChatMistralAI(model=\"mistral-large-latest\", temperature =0)"
      ],
      "metadata": {
        "id": "Wi07eInPBvki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to ingest Appwall security and system DBs in this example and return pandas dataframes"
      ],
      "metadata": {
        "id": "t0NFq-7fCti9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def epochtotzone(ep,tzo= None):\n",
        "  \"\"\"\n",
        "  Convert epoch values in db to DateTime\n",
        "  \"\"\"\n",
        "  if not tzo:\n",
        "    tzo = \"Asia/Jerusalem\"\n",
        "  tzn=timezone(tzo)\n",
        "  try:\n",
        "      t=datetime.datetime.utcfromtimestamp(ep).replace(tzinfo=pytz.utc)\n",
        "      x= t.astimezone(tzn)\n",
        "      return t.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
        "\n",
        "\n",
        "def aw_to_pd(f,ftype='security',tzone=None):\n",
        "  \"\"\"\n",
        "  Appwall security and system db to pandas dataframe\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  jsonlist=[]\n",
        "\n",
        "  if 'security' in ftype:\n",
        "      colls= ['DateTime','TargetPort','TargetIP','TransID','TunnelID','IsPassiveMode','Title','URI','Description']\n",
        "  elif 'system' in ftype:\n",
        "      colls=['DateTime','Title','Description']\n",
        "\n",
        "  con = sqlite3.connect(f)\n",
        "  con.text_factory = lambda b: b.decode(errors = 'ignore') # added to ignore utf-8 decode failure\n",
        "  cursor = con.cursor()\n",
        "  cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "  qu='SELECT * from %s'%'Events'\n",
        "  df=pd.read_sql(qu,con)\n",
        "  df=df[~df['DateTime'].isna()]\n",
        "  df['DateTime']=df['DateTime'].apply(epochtotzone,args=(tzone,))\n",
        "  df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
        "  df=df[colls]\n",
        "  df['Date']= df['DateTime']\n",
        "  df = df.drop('DateTime', axis=1, inplace=False)\n",
        "  df['Date'] = df['Date'].astype(str)\n",
        "  if 'security' in ftype:\n",
        "      df['TransID'] = df['TransID'].astype(str)\n",
        "  return df\n",
        "\n",
        "gdrivepath = \"/content/drive/MyDrive/Colab Notebooks/My NN stuff/alteon/dbe.security.db\"\n",
        "\n",
        "df_aw = aw_to_pd(gdrivepath)\n",
        "\n"
      ],
      "metadata": {
        "id": "A1TH1CHXBzDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting up langchain components and pipeline**"
      ],
      "metadata": {
        "id": "7-gsZ2jKC7hU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python_repl_appwall = PythonREPL()\n",
        "python_repl_appwall.globals['df'] = df_aw\n",
        "\n",
        "\n",
        "# Permanently changes the pandas settings\n",
        "python_repl_appwall.run(pd.set_option('display.max_rows', None))\n",
        "python_repl_appwall.run(pd.set_option('display.max_columns', None))\n",
        "python_repl_appwall.run(pd.set_option('display.width', None))\n",
        "python_repl_appwall.run(pd.set_option('display.max_colwidth', 500))\n",
        "\n",
        "\n",
        "name_appwall = \"python_repl_for_pandas_dataframes_appwall_database\"\n",
        "\n",
        "description_appwall_2= \"\"\"\n",
        "A Python shell to execute pandas commands on appwall dataframe 'df' which contains appwall database (db).\n",
        "The column names in this pandas dataframe df can be found in this list = ['Date','TargetPort','TargetIP','TransID','TunnelID','IsPassiveMode','Title','URI','Description']\n",
        "The dataframe df contains entries in increasing order of time,\n",
        "such that the latest entry is at the end and earliest entry is at the beginning.\n",
        "The final answer should always be enclosed in a print() function\n",
        "\"\"\"\n",
        "\n",
        "repl_tool_appwall = Tool(\n",
        "    name=name_appwall,\n",
        "    description=description_appwall_2,\n",
        "    func=python_repl_appwall.run\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "appwall_llm = llmMist.bind_tools([repl_tool_appwall])\n",
        "\n",
        "sys_appwall= \"\"\"\n",
        "Only if user query is talking about 'appwall' or 'WAF' or 'AW' or 'CWAF' or 'database' or 'db',\n",
        "do they have questions about the data stored in the pandas dataframe df for appwall database.\n",
        "\"\"\"\n",
        "\n",
        "prompt_appwall = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", sys_appwall),\n",
        "        (\"human\", \"{query}\"),\n",
        "        ])\n",
        "\n",
        "setupinputs = {\"query\": RunnablePassthrough()}\n",
        "\n",
        "\n",
        "def check_print(result: str) -> str:\n",
        "  if 'print' in result:\n",
        "    return result\n",
        "  else:\n",
        "    return f\"print({result})\"\n",
        "\n",
        "def out_parser_appwall(result: str) -> str:\n",
        "\n",
        "  \"\"\"Parses the output of python repl and returns the final formatted output\"\"\"\n",
        "\n",
        "  no_answer = \"I couldnt locate what you are asking for in the appwall db.\"\n",
        "  attributeerr = \"I dont have an answer to your query. Please try to revise your question and ask again. \"\n",
        "  if not result:\n",
        "    return no_answer\n",
        "  if 'Empty DataFrame' in result:\n",
        "    return no_answer\n",
        "  if 'AttributeError' in result:\n",
        "    return attributeerr\n",
        "  return result\n",
        "\n",
        "def toolingfunc_appwall(x: str) -> str:\n",
        "  try:\n",
        "    return x.tool_calls[0][\"args\"]['__arg1']\n",
        "  except:\n",
        "    # No tool/arg found. This need to be different message encouraging user to rephrase the question\n",
        "    return \"print('I dont have an answer to your query. Kindly try rephrasing your question or adding more specificity to it.')\"\n",
        "\n",
        "\n",
        "def notebook_rate_print(data: str) -> str:\n",
        "  if not data:\n",
        "    return data\n",
        "  data_size = len(data)\n",
        "  if data_size > 999000:\n",
        "    return \"Response data size exceeds display limit and will only be partially displayed. Please refine your query and ask again!\\n\\n\" +data[:999000]\n",
        "  else:\n",
        "    return data\n",
        "\n",
        "appwall_chain = setupinputs| prompt_appwall | appwall_llm | RunnableLambda(toolingfunc_appwall) | RunnableLambda(check_print) | python_repl_appwall.run | RunnableLambda(notebook_rate_print) | RunnableLambda(out_parser_appwall)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HT9Yje1_CSRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Call chain with your questions**"
      ],
      "metadata": {
        "id": "EpW-_k-TDHFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"query: \", query)\n",
        "resp = appwall_chain.invoke({'query':query})\n",
        "print(resp)\n"
      ],
      "metadata": {
        "id": "hHRWn0rRDGpj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
