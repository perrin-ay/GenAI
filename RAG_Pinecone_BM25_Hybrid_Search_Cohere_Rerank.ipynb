{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi73cBL6JRmt"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade google-cloud-aiplatform\n",
        "!pip install pinecone-client\n",
        "import sys\n",
        "!{sys.executable} -m pip install langchain huggingface_hub\n",
        "!{sys.executable} -m  pip install bitsandbytes>=0.39.0\n",
        "!{sys.executable} -m pip install --upgrade accelerate\n",
        "!{sys.executable} -m pip install --upgrade sentence_transformers\n",
        "!{sys.executable} -m  pip install --upgrade transformers\n",
        "!{sys.executable} -m pip install --upgrade trl\n",
        "!{sys.executable} -m pip install --upgrade peft\n",
        "!{sys.executable} -m pip install --upgrade sqlite3\n",
        "!{sys.executable} -m pip install --upgrade pytz\n",
        "!{sys.executable} -m pip install --upgrade pypdf PyPDF2\n",
        "!{sys.executable} -m pip install --upgrade langchain_experimental\n",
        "!{sys.executable} -m pip install --upgrade langchain_community\n",
        "!{sys.executable} -m pip install --upgrade langchain_openai\n",
        "!{sys.executable} -m pip install --upgrade openai tiktoken chromadb\n",
        "!pip install unstructured['pdf'] unstructured\n",
        "!{sys.executable} -m pip install --upgrade textwrap\n",
        "!pip install --upgrade --quiet  cohere\n",
        "!pip install langchain-cohere\n",
        "!pip install grandalf\n",
        "!pip install wikipedia\n",
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu\n",
        "!pip install langchainhub\n",
        "!pip install -qU langchain-mistralai\n",
        "!pip install \"unstructured[html]\"\n",
        "!pip install bs4\n",
        "!pip install rank_bm25\n",
        "!pip install gradio\n",
        "!pip install langchain_chroma\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "import os\n",
        "import time\n",
        "import sqlite3\n",
        "import re\n",
        "from pytz import timezone\n",
        "import pytz\n",
        "import datetime\n",
        "import json\n",
        "import textwrap\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch\n",
        "from operator import itemgetter\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import peft\n",
        "import bitsandbytes\n",
        "import accelerate\n",
        "import trl\n",
        "from accelerate import init_empty_weights\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoConfig, BitsAndBytesConfig\n",
        "from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model\n",
        "from accelerate import load_checkpoint_and_dispatch\n",
        "import accelerate, bitsandbytes\n",
        "\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "import cohere\n",
        "\n",
        "from langchain import hub\n",
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "from langchain_experimental.agents import create_csv_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, PyMuPDFLoader\n",
        "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain.tools import BaseTool, StructuredTool, tool\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain.agents import create_tool_calling_agent\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain.agents import AgentType, initialize_agent\n",
        "from langchain_core.messages import HumanMessage, ToolMessage, SystemMessage, AIMessage\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        ")\n",
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n"
      ],
      "metadata": {
        "id": "fVcwd-zJJtKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Api keys setup**"
      ],
      "metadata": {
        "id": "BkU9X8y2J-Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Mistral_TOKEN = userdata.get('mistralapi')\n",
        "os.environ['MISTRAL_API_KEY'] = Mistral_TOKEN\n",
        "\n",
        "co = cohere.Client(coherekey)\n",
        "os.environ[\"COHERE_API_KEY\"] = coherekey\n",
        "\n",
        "\n",
        "pckey = userdata.get('pinecone')\n",
        "pc = Pinecone(api_key=pckey)"
      ],
      "metadata": {
        "id": "h-tq8sXhJ-F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instantiate your pinecone index**"
      ],
      "metadata": {
        "id": "MResokBdJ4zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"rag\"\n",
        "index = pc.Index(index_name)\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "id": "XWQdQvm-JxrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings model**"
      ],
      "metadata": {
        "id": "g-9uOwmTKWNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
        "model_kwargs = {'device': 'cpu', 'trust_remote_code': True}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "### NOTE dim size is 1024 NOT 768 ########\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "p59nVeyrJxuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pinecone retriever**"
      ],
      "metadata": {
        "id": "xJF2Bn-YKayW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pineconeretriever(query, top_k=12, namespace='ALTEON_CLI_GUIDE', returnLCDocument=True):\n",
        "\n",
        "  contexts = []\n",
        "  citations = []\n",
        "\n",
        "  if not query:\n",
        "      return [\"No question was submitted so no context was retrieved\"],['No citations']\n",
        "  question = query\n",
        "\n",
        "\n",
        "  res = index.query(namespace=namespace,vector=hf.embed_query(question), top_k=top_k, include_metadata = True )\n",
        "  contexts = contexts + [\n",
        "      Document(page_content = x['metadata']['text'], metadata ={'source': x['metadata']['source'] }) if returnLCDocument else x['metadata']['text'] for x in res['matches']\n",
        "  ]\n",
        "  citations = citations + [\n",
        "      x['metadata']['source'] for x in res['matches']\n",
        "  ]\n",
        "  return contexts, citations"
      ],
      "metadata": {
        "id": "h44Y39HKKaMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mistral used as the LLM here**"
      ],
      "metadata": {
        "id": "-_n6znrhKnEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llmMist = ChatMistralAI(model=\"mistral-large-latest\", temperature =0)"
      ],
      "metadata": {
        "id": "hMdqq2knKaSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cohere Reranker**"
      ],
      "metadata": {
        "id": "jrdV9s7EKzNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_coherererank( query ='', documents =[],top_n = 5,returnLCDocument= True):\n",
        "\n",
        "  if len(documents) == 0:\n",
        "      return []\n",
        "  docs = [\n",
        "      doc.page_content if isinstance(doc, Document) else doc for doc in documents\n",
        "  ]\n",
        "  results = co.rerank(query=query, documents=docs, top_n= top_n, model='rerank-english-v3.0', return_documents= False)\n",
        "  if hasattr(results, \"results\"):\n",
        "      results = getattr(results, \"results\") # returns list of results\n",
        "  result_dicts = []\n",
        "  for res in results:\n",
        "      result_dicts.append(\n",
        "          {\"index\": res.index, \"relevance_score\": res.relevance_score}\n",
        "      )\n",
        "\n",
        "  compressed = []\n",
        "\n",
        "  for i in result_dicts:\n",
        "      doc_copy = docs[i[\"index\"]]\n",
        "      if returnLCDocument: # convert to Document type\n",
        "        doc_copy = Document(page_content=doc_copy, metadata=documents[i[\"index\"]].metadata) if isinstance(documents[0], Document) else Document(page_content=doc_copy)\n",
        "        doc_copy.metadata[\"relevance_score\"] = i[\"relevance_score\"]\n",
        "        doc_copy.metadata[\"order_idx\"] = i[\"index\"]\n",
        "      compressed.append(doc_copy)\n",
        "\n",
        "  return compressed"
      ],
      "metadata": {
        "id": "4f8wXw4sKaVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG pipeline and QA**"
      ],
      "metadata": {
        "id": "RqPCBVS7LAqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### template ####\n",
        "\n",
        "template = \"\"\"\n",
        "Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "\n",
        "{context}\n",
        "\n",
        "</context>\n",
        "\n",
        "Question: {query}\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"query\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "\n",
        "def format_chunks(docs):\n",
        "  \"\"\"\n",
        "  Format the chunks for the prompt context\n",
        "  \"\"\"\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs) if isinstance(docs[0], Document) else \"\\n\\n\".join(docs)\n",
        "\n",
        "query ='How to disable management access to data ports ?'\n",
        "print (\"query: \", query)\n",
        "\n",
        "### vector retrieval\n",
        "context, citations = pineconeretriever(query, top_k=12, namespace='ALTEON_CLI_GUIDE', returnLCDocument=True)\n",
        "\n",
        "##### ensemble retrieval\n",
        "retrieverbm25 = BM25Retriever.from_texts([\"Arnab says to enable data port with management access use /c/sys/ports/ena\"])\n",
        "retrieverbm25.k = 4\n",
        "ensemble_retriever_cliguide = EnsembleRetriever(retrievers=[RunnableLambda(lambda query: context), retrieverbm25 ],weights=[0.8, 0.2])\n",
        "ensemblecontext = ensemble_retriever_cliguide.invoke(query)\n",
        "\n",
        "### rerank\n",
        "compressed_context = do_coherererank( query =query, documents = ensemblecontext,top_n = 5,returnLCDocument= True)\n",
        "\n",
        "#### fill prompt with query and compressed context\n",
        "filledprompt = prompt.format(context=format_chunks(compressed_context), query=query)\n",
        "\n",
        "\n",
        "#### call mistral llm\n",
        "response = llmMist.invoke(filledprompt)\n",
        "print (\"response\", response.content)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AcX95398Jxww"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}