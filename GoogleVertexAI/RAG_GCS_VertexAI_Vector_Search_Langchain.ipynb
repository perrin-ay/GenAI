{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8Ao8-XhhgTK"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-secret-manager\n",
        "!pip install --upgrade google-cloud-aiplatform\n",
        "!pip install google-cloud-secret-manager"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install langchain huggingface_hub\n",
        "\n",
        "!{sys.executable} -m  pip install bitsandbytes>=0.39.0\n",
        "!{sys.executable} -m pip install --upgrade accelerate\n",
        "!{sys.executable} -m pip install --upgrade sentence_transformers\n",
        "!{sys.executable} -m  pip install --upgrade transformers\n",
        "!{sys.executable} -m pip install --upgrade trl\n",
        "!{sys.executable} -m pip install --upgrade peft\n",
        "!{sys.executable} -m pip install --upgrade sqlite3\n",
        "!{sys.executable} -m pip install --upgrade pytz\n",
        "!{sys.executable} -m pip install --upgrade pypdf PyPDF2\n",
        "!{sys.executable} -m pip install --upgrade langchain_experimental\n",
        "!{sys.executable} -m pip install --upgrade langchain_community\n",
        "!{sys.executable} -m pip install --upgrade langchain_openai\n",
        "!{sys.executable} -m pip install --upgrade openai tiktoken chromadb\n",
        "!pip install unstructured['pdf'] unstructured\n",
        "!{sys.executable} -m pip install --upgrade textwrap\n",
        "!pip install --upgrade --quiet  cohere\n",
        "!pip install langchain-cohere\n",
        "!pip install grandalf\n",
        "!pip install wikipedia\n",
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu\n",
        "!pip install langchainhub\n",
        "!pip install -qU langchain-mistralai\n",
        "!pip install \"unstructured[html]\"\n",
        "!pip install bs4\n",
        "!pip install rank_bm25\n",
        "!pip install gradio\n",
        "!pip install langchain_chroma\n",
        "\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "import os\n",
        "import time\n",
        "import sqlite3\n",
        "import re\n",
        "from pytz import timezone\n",
        "import pytz\n",
        "import datetime\n",
        "import json\n",
        "import textwrap\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch\n",
        "from operator import itemgetter\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import peft\n",
        "import bitsandbytes\n",
        "import accelerate\n",
        "import trl\n",
        "from accelerate import init_empty_weights\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoConfig, BitsAndBytesConfig\n",
        "from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model\n",
        "from accelerate import load_checkpoint_and_dispatch\n",
        "import accelerate, bitsandbytes\n",
        "\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "from langchain import hub\n",
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "from langchain_experimental.agents import create_csv_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, PyMuPDFLoader\n",
        "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain.tools import BaseTool, StructuredTool, tool\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain.agents import create_tool_calling_agent\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain.agents import AgentType, initialize_agent\n",
        "from langchain_core.messages import HumanMessage, ToolMessage, SystemMessage, AIMessage\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        ")\n",
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "zBwbBF0kwqOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from google.cloud import aiplatform\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part\n",
        "import vertexai.preview.generative_models as generative_models\n",
        "from vertexai.language_models import TextGenerationModel, ChatModel, InputOutputTextPair\n",
        "\n",
        "import getpass\n",
        "from google.colab import userdata\n",
        "import base64\n",
        "import vertexai\n",
        "from typing import List, Optional\n",
        "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
        "from vertexai.generative_models import GenerativeModel,GenerationConfig, Part, FinishReason, FunctionDeclaration,Tool,HarmBlockThreshold\n",
        "from vertexai.generative_models import HarmCategory, grounding, GenerationResponse\n",
        "import vertexai.preview.generative_models as generative_models\n",
        "from vertexai.preview.generative_models import grounding as preview_grounding\n",
        "from google.cloud import aiplatform"
      ],
      "metadata": {
        "id": "GdefKhSEjnR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings model**"
      ],
      "metadata": {
        "id": "xY3AaxlqidVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
        "model_kwargs = {'device': 'cpu', 'trust_remote_code': True}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cps6ChNMiPO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Search endpoint**"
      ],
      "metadata": {
        "id": "UmJ736WTjuxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = \"projects/930596367380/locations/us-central1/indexEndpoints/6368806754683191296\" # copy fullqualified name from console\n",
        "new_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(endpoint)\n"
      ],
      "metadata": {
        "id": "VOME1_E0juSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to return sources**"
      ],
      "metadata": {
        "id": "Z9G2Hnssib4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filenameRegistry(name):\n",
        "  name = name.split('/')[0]\n",
        "  gcsname_to_filename = {'cliguide_chunks':\"Alteon CLI User's Guide.pdf\"}\n",
        "  gcsname_to_link = {'cliguide_chunks':\"https://storage.googleapis.com/bucker_uscentral/cliguide_chunks/Alteon CLI User's Guide.pdf\"}\n",
        "  try:\n",
        "    return [gcsname_to_filename[name], gcsname_to_link[name]]\n",
        "  except:\n",
        "    return ['error retrieving filename','error retrieving filename' ]\n"
      ],
      "metadata": {
        "id": "nlJqhJ_xibaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemini LLM**"
      ],
      "metadata": {
        "id": "qTUqey52udDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_ID = \"gemini-1.5-flash-001\"\n",
        "model = GenerativeModel(MODEL_ID)\n",
        "\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    candidate_count=1, #The number of response variations to return. This value must be 1\n",
        "    max_output_tokens=8192,\n",
        ")\n",
        "\n",
        "nosafetysettings = {\n",
        "#    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE, # only for monthly invoiced billed account\n",
        "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "}\n",
        "\n",
        "\n",
        "def llmgem(prompt):\n",
        "  response = model.generate_content(\n",
        "    prompt, # contents can be supplied here as well which is prompt sent as a single text element in a list\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=nosafetysettings,\n",
        ")\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "7lA8sgXducWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function for semantic search using vertex ai vector search and retriver of chunks**"
      ],
      "metadata": {
        "id": "3HTNIEbIueLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "from google.cloud import storage\n",
        "import io\n",
        "\n",
        "def download_blob_to_memory(blob_name, bucket_name =\"bucker_uscentral\"):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "\n",
        "    in_memory_file = io.BytesIO()\n",
        "    blob.download_to_file(in_memory_file)\n",
        "    in_memory_file.seek(0)\n",
        "    return blob_name, in_memory_file\n",
        "\n",
        "def downloadFilesMemory(blob_name, bucket_name =\"bucker_uscentral\"):\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    contents = blob.download_as_bytes()\n",
        "\n",
        "    return blob_name, contents.decode(\"utf-8\")\n",
        "\n",
        "def download_files_concurrently(file_list,bucket_name =\"bucker_uscentral\", max_workers=5):\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers= max_workers) as executor:\n",
        "        futures = [\n",
        "            executor.submit(download_blob_to_memory, file_name,bucket_name  )\n",
        "            for file_name in file_list\n",
        "        ]\n",
        "\n",
        "        results = {}\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            blob_name, in_memory_file = future.result()\n",
        "            results[blob_name] = in_memory_file\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "class vsearchretriever (object):\n",
        "  def __init__(self,index_endpoint, index_id = \"index_dims1024_stream\",num_neighbors=10):\n",
        "    self.index_endpoint = index_endpoint\n",
        "    self.index_id = index_id\n",
        "    self.num_neighbors = num_neighbors\n",
        "    self.cites = []\n",
        "\n",
        "  def _get_vec_neighbors(self, query):\n",
        "    embeddings = hf.embed_query(query)\n",
        "    self.response = self.index_endpoint.find_neighbors(\n",
        "        deployed_index_id=self.index_id ,\n",
        "        queries=[embeddings],\n",
        "        num_neighbors= self.num_neighbors,\n",
        "    )\n",
        "    return self.response\n",
        "\n",
        "  def get_docs(self, query):\n",
        "    self.response = self._get_vec_neighbors(query)\n",
        "    dictforcontent = {'id':[],'distance':[]}\n",
        "    for i in self.response[0]:\n",
        "      dictforcontent['id'].append(i.id)\n",
        "      dictforcontent['distance'].append(i.distance)\n",
        "    orderedids = dictforcontent['id']\n",
        "    newdict = {}\n",
        "    self.context = []\n",
        "    self.cites = []\n",
        "    downloaded_files = download_files_concurrently(dictforcontent['id'])\n",
        "    for blob_name, in_memory_file in downloaded_files.items():\n",
        "        content = in_memory_file.getvalue().decode(\"utf-8\")\n",
        "        newdict[blob_name] = Document(page_content=content, metadata={\"source\": filenameRegistry(blob_name)[0]})\n",
        "        self.cites.append(filenameRegistry(blob_name)[0])\n",
        "    for i in orderedids:\n",
        "      self.context.append(newdict[i]) # added for langc documents\n",
        "\n",
        "    self.cites = list(set(self.cites)) # remove duplicates\n",
        "\n",
        "    return self.context # for LC we need to return a list [Documents(),..]\n"
      ],
      "metadata": {
        "id": "jFA6x0S5ucUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instantiate BM25 for hybrid retreival and semantic search retriever using above function**"
      ],
      "metadata": {
        "id": "ANCE_aVWuemn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "retrieverbm25 = BM25Retriever.from_texts([\"Arnab says to enable data port with management access use /c/sys/ports/ena\"])\n",
        "retrieverbm25.k = 4\n",
        "retriever = vsearchretriever(new_index_endpoint,index_id = \"index_dims1024_stream\", num_neighbors=10)\n",
        "retrieverembed = retriever.get_docs"
      ],
      "metadata": {
        "id": "wNYPiXkbucK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final RAG pipeline with everything in a langchain chain**"
      ],
      "metadata": {
        "id": "RkXM2jTUv5Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##### template ####\n",
        "\n",
        "template = \"\"\"\n",
        "Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "\n",
        "{context}\n",
        "\n",
        "</context>\n",
        "\n",
        "Question: {query}\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"query\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "\n",
        "def lcprompt(prompt):\n",
        "  return prompt.to_string()\n",
        "\n",
        "def format_chunks(docs):\n",
        "  \"\"\"\n",
        "  Format the chunks for the prompt context\n",
        "  \"\"\"\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\",top_n=7) # topn 5 was producing better results on some questions\n",
        "\n",
        "#### ensemble retriever for semantic and keyword search ###########\n",
        "ensemble_retriever_cliguide = EnsembleRetriever(retrievers=[RunnableLambda(retrieverembed),retrieverbm25],weights=[0.8, 0.2])\n",
        "\n",
        "######## cohere reranker#####################\n",
        "compression_retriever_cliguide = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=ensemble_retriever_cliguide\n",
        ")\n",
        "\n",
        "#### langchain LCEL ###########################\n",
        "setupandret = RunnableParallel({\"context\": compression_retriever_cliguide | format_chunks , \"query\": RunnablePassthrough()})\n",
        "chain_setup = setupandret\n",
        "chain_prompt = setupandret | prompt | RunnableLambda(lcprompt)\n",
        "chain = setupandret | prompt | RunnableLambda(lcprompt) | RunnableLambda(llmgem)\n",
        "query ='How to disable management access to data ports ?'\n",
        "print (\"query: \", query)\n",
        "print (chain.invoke(query) + \"\\n\\nSources:\\n\\n\" + \"\\n\".join(retriever.cites))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6rLJ1L4Dv4m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OrFqQljnv4pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Ldk2pqov4sB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}